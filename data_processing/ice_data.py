"""
DataLoader for ICE-BENCH

"""

import os
import torch
from torch.utils.data import Dataset
import cv2
import albumentations as A


class IceDataset(Dataset):
    """

    This class loads a dataset of preprocessed SAR images and their corresponding masks produced by C. Baumhoer.

    Args:
        - mode (str): 'train', 'val', or 'test'
        - parent_dir (str): Root directory containing 'scenes', 'masks' folders.
        - augment: whether to apply data augmentation (true for train, false for val/test)
        
    Adapt this to take into account the different structure of the test data:
        - train/val: /parent_dir/train/scenes, /parent_dir/train/masks
        - test: /parent_dir/satellite/test_satellite/scenes, /parent_dir/satellite/test_satellite/masks
         
    """

    def __init__(self, mode, parent_dir, satellite=None, augment=True):
        print(f"Initializing IceDataset in {mode} mode...")

        self.mode = mode
        self.parent_dir = parent_dir
        self.satellite = satellite

        if augment is None:
            self.augment = mode == "train"
        else:
            self.augment = augment

        if mode == "test":
            self.image_dir = os.path.join(parent_dir, "preprocessed_data", "test", "images")
            self.mask_dir = os.path.join(parent_dir, "preprocessed_data", "test", "masks")

        else:
            # train/val
            self.image_dir = os.path.join(parent_dir, mode, "images")
            self.mask_dir = os.path.join(parent_dir, mode, "masks")
            
        # Verify directories exist
        if not os.path.exists(self.image_dir):
            raise ValueError(f"Image directory does not exist: {self.image_dir}")
        if not os.path.exists(self.mask_dir):
            raise ValueError(f"Mask directory does not exist: {self.mask_dir}")



        # this only works on preprocessed images 
        # Get all image files (lazy loading - just get count and verify structure)
        self.image_files = sorted(
            [f for f in os.listdir(self.image_dir) if f.endswith((".png"))]
        )
        self.mask_files = sorted(
            [f for f in os.listdir(self.mask_dir) if f.endswith((".png"))]
        )

        print(f"Found {len(self.image_files)} images and {len(self.mask_files)} masks")
        print(f"{mode} {satellite or ''}: {len(self.image_files)} samples")

        if len(self.image_files) != len(self.mask_files):
            print(
                f"Warning: Mismatch in number of images ({len(self.image_files)}) and masks ({len(self.mask_files)})"
            )
            # Take the minimum to avoid index errors
            min_count = min(len(self.image_files), len(self.mask_files))
            self.image_files = self.image_files[:min_count]
            self.mask_files = self.mask_files[:min_count]
            print(f"Using {min_count} image-mask pairs")

        # Data augmentation transforms (only applied during training)
        if self.augment:
            self.transform = A.Compose(
                [
                    A.RandomRotate90(p=0.5),
                    A.HorizontalFlip(p=0.5),
                    A.VerticalFlip(p=0.3),
                    A.OneOf(
                        [
                            A.RandomBrightnessContrast(
                                brightness_limit=0.2, contrast_limit=0.2, p=1.0
                            ),
                            A.GaussNoise()#var_limit=(10.0, 50.0), p=1.0),
                        ],
                        p=0.3,
                    ),
                    # Optional: Add more SAR-specific augmentations
                    A.RandomGamma(gamma_limit=(80, 120), p=0.2),
                ]
            )
        else:
            # No transforms for validation
            self.transform = A.Compose([])

        # TODO: Check this
        # Separate normalization for image only (after joint transformations)
        self.normalize = A.Normalize(mean=0.3047126829624176, std=0.32187142968177795)

    def __len__(self):
        """
        Returns the number of images in the dataset.
        """
        return len(self.image_files)

    def __getitem__(self, idx):
        """ "
        Loads a SAR image, and its corresponding label
        """
        image_name = self.image_files[idx]
        mask_name = self.mask_files[idx]

        # Load image with OpenCV and convert to PIL Image
        image_path = os.path.join(self.image_dir, image_name)
        mask_path = os.path.join(self.mask_dir, mask_name)

        # Load image and mask
        image_np = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        mask_np = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        if image_np is None:
            raise ValueError(f"Could not load image: {image_path}")
        if mask_np is None:
            raise ValueError(f"Could not load mask: {mask_path}")

        # TODO: up to here (2.7.25), check tensor and normalisation things

        # Apply same spatial transformations to both augmentation
        transformed = self.transform(image=image_np, mask=mask_np)
        image_transformed, mask_transformed = transformed["image"], transformed["mask"]

        # TODO: I have already normalised the images so decide whether to keep
        # Apply normalization only to image
        image_normalized = self.normalize(image=image_transformed)["image"]

        image_tensor = torch.from_numpy(image_normalized).float().unsqueeze(0)

        # mask already black and white
        mask_tensor = torch.from_numpy(mask_transformed).float().unsqueeze(0)

        return image_tensor, mask_tensor
    
    
    #debug
    @staticmethod
    def create_test_datasets(parent_dir):
        """
        Create test dataset from the unified test directory
        Since all test data is now in one location, we create a single dataset
        """
        # Debug: Print expected path structure
        expected_test_dir = os.path.join(parent_dir, "preprocessed_data", "test")
        expected_images_dir = os.path.join(expected_test_dir, "images")
        expected_masks_dir = os.path.join(expected_test_dir, "masks")
        
        print(f"Debug: Looking for test data at:")
        print(f"  Images: {expected_images_dir}")
        print(f"  Masks: {expected_masks_dir}")
        print(f"  Images exists: {os.path.exists(expected_images_dir)}")
        print(f"  Masks exists: {os.path.exists(expected_masks_dir)}")
        
        if os.path.exists(expected_images_dir):
            image_files = os.listdir(expected_images_dir)
            png_files = [f for f in image_files if f.endswith('.png')]
            print(f"  All files in images: {len(image_files)}")
            print(f"  PNG files in images: {len(png_files)}")
            if len(image_files) > 0:
                print(f"  Sample files: {image_files[:5]}")
                
        if os.path.exists(expected_masks_dir):
            mask_files = os.listdir(expected_masks_dir)
            png_mask_files = [f for f in mask_files if f.endswith('.png')]
            print(f"  All files in masks: {len(mask_files)}")
            print(f"  PNG files in masks: {len(png_mask_files)}")
        
        test_datasets = {}
        
        try:
            # Create a single test dataset
            print("Creating test dataset...")
            test_dataset = IceDataset('test', parent_dir, satellite=None, augment=False)
            
            # Only create satellite references if the dataset has data
            if len(test_dataset) > 0:
                satellites = ['ERS', 'Envisat', 'Sentinel-1']
                for satellite in satellites:
                    test_datasets[satellite] = test_dataset
                print(f"Successfully created test dataset with {len(test_dataset)} samples")
            else:
                print("Warning: Test dataset is empty")
                
        except Exception as e:
            print(f"Could not load test dataset: {e}")
            import traceback
            traceback.print_exc()
        
        return test_datasets
    
    # @staticmethod
    # def create_test_datasets(parent_dir):
    #     """
    #     Create only test datasets
    #     """
    #     test_datasets = {}
        
    #     try:
    #         # Create a single test dataset since all satellites are combined
    #         test_dataset = IceDataset('test', parent_dir, satellite=None, augment=False)
            
    #         # For compatibility with existing code that expects satellite-specific datasets,
    #         # we can create the same dataset reference for each satellite name
    #         satellites = ['ERS', 'Envisat', 'Sentinel-1']
    #         for satellite in satellites:
    #             test_datasets[satellite] = test_dataset
                
    #     except Exception as e:
    #         print(f"Could not load test dataset: {e}")
        
    #     return test_datasets
